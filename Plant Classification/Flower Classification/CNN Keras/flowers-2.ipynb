{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n    \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categories=['dandelion', 'daisy', 'sunflower', 'tulip', 'rose']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# initializing the working directory","metadata":{}},{"cell_type":"code","source":"dire='/kaggle/input/flowers-recognition/flowers'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Loading Data","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfeatures=[]\nfor i in categories:\n    path=os.path.join(dire,i)\n    num_classes=categories.index(i)\n    for img in os.listdir(path):\n        if img.endswith('.jpg'):\n            \n            img_array=cv2.imread(os.path.join(path,img),cv2.IMREAD_COLOR)\n            img_array=cv2.resize(img_array,(150,150))\n            features.append([img_array,num_classes])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"X=[]\ny=[]\nfor i,j in features:\n    X.append(i)\n    y.append(j)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# visualizations: lets have a look on our dataset","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfig,ax=plt.subplots(5,2)\nfig.set_size_inches(15,15)\nfor i in range(5):\n    for j in range (2):\n        l=np.random.randint(0,len(y))\n        ax[i,j].imshow(X[l])\n        ax[i,j].set_title('Flower: '+categories[y[l]])\nplt.axis('off')        \nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reshaping and normalizing: the need of resizing the feature vetcor x is to meet the keras requirement and normalization is done to scale all the values in a similar range","metadata":{}},{"cell_type":"code","source":"X=np.array(X).reshape(-1,150,150,3)/255.0\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  lets see how our class lables are distributed","metadata":{}},{"cell_type":"code","source":"sns.set_style('whitegrid')\nplt.figure(figsize=(14,7))\nfig=sns.countplot(y)\nfig.set(xticks=range(len(categories)), xticklabels=[i for i in categories])\nplt.xlabel('FLOWER SPECIES')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_dandelion=len([i for i in y if i==0])\nlist_daisy=len([i for i in y if i==1])\nlist_sunflower=len([i for i in y if i==2])\nlist_tulip=len([ i for i in y if i==3])\nlist_rose=len([i for i in y if i==4])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_species=[list_dandelion,list_daisy,list_sunflower,list_tulip,list_rose]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style('whitegrid')\nplt.figure(figsize=(18,10))\nplt.pie(list_species,labels=categories,startangle=90,colors=['r','g','b','y','m'],autopct='%1.1f%%',explode = (0, 0.1, 0, 0,0),shadow=True)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\ny=to_categorical(y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model building","metadata":{}},{"cell_type":"code","source":"from keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.utils import to_categorical\n\n# specifically for cnn\nfrom keras.layers import Dropout, Flatten,Activation\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n \nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), input_shape=(150,150,3)))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(2, 2, padding=\"same\"))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(2, 2, padding=\"same\"))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(128, (3, 3)))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(2, 2, padding=\"same\"))\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation=\"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128, activation=\"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(5, activation=\"softmax\"))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs=50\n\nfrom keras.callbacks import ReduceLROnPlateau\nred_lr= ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> # Data Augmentation","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=128),\n                              epochs = epochs, validation_data = \n                              (x_test,y_test),\n                              verbose = 1, steps_per_epoch=x_train.shape[0] // 128)\n# model.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data = (x_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# lets visualize our model functioning","metadata":{}},{"cell_type":"code","source":"sns.set_style('whitegrid')\nplt.figure(figsize=(12,5))\nplt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style('whitegrid')\nplt.figure(figsize=(12,5))\nplt.plot(History.history['accuracy'])\nplt.plot(History.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Its time for prediction ","metadata":{}},{"cell_type":"code","source":"preds=model.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=np.argmax(preds,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correct_class=[]\nincorrect_class=[]\ni=0\nfor i in range(len(y_test)):\n    if(np.argmax(y_test[i])==predictions[i]):\n        correct_class.append(i)\n    if(len(correct_class)==8):\n        break\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=0\nfor i in range(len(y_test)):\n    \n    if (np.argmax(y_test[i])!=predictions[i]):\n        \n        incorrect_class.append(i)\n    if (len(incorrect_class)==8):\n        break\n        \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysing the predicitons","metadata":{}},{"cell_type":"code","source":"count=0\nfig,ax=plt.subplots(4,2)\nfig.set_size_inches(15,15)\nfor i in range (4):\n    for j in range (2):\n        ax[i,j].imshow(x_test[correct_class[count]])\n        ax[i,j].set_title(\"Predicted Flower : \"+ categories[predictions[correct_class[count]]] +\"\\n\"+\"Actual Flower : \"+ categories[np.argmax(y_test[correct_class[count]])])\n        plt.tight_layout()\n        count+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=0\nfig,ax=plt.subplots(4,2)\nfig.set_size_inches(15,15)\nfor i in range(4):\n    for j in range(2):\n        ax[i,j].imshow(x_test[incorrect_class[count]])\n        ax[i,j].set_title(\"Predicted flower : \" + categories[predictions[incorrect_class[count]]] + \"\\n\"+\"Actual Flower : \" +categories[np.argmax(y_test[incorrect_class[count]])])\n        plt.tight_layout()\n        count+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiment and Fun","metadata":{}},{"cell_type":"markdown","source":"> Well this is for experiment and have fun with the model you have built.You can load images from the internet and predict using the code written below","metadata":{}},{"cell_type":"markdown","source":"> The image we want to predict is also needed to be preprocessed according to the requirements of the model.You need to take care of resizing  like we did below to resize it in (150,150) dimensions","metadata":{}},{"cell_type":"code","source":"import requests\nfrom PIL import Image\nfrom io import BytesIO\n\ndef process_image(url):\n    response=requests.get(url)\n    img=Image.open(BytesIO(response.content))\n    fix,ax=plt.subplots(1,3,figsize=(15,20))\n    ax[0].imshow(img)\n    ax[0].set_title('image')\n    \n    #grayscale and normalization\n    img=np.array(img)\n    img=cv2.cvtColor(img,cv2.IMREAD_COLOR)\n    print(img.shape)\n    img=img/255.0\n    ax[1].imshow(img)\n    ax[1].set_title('color image')\n    \n    #resizing\n    img=cv2.resize(img,(150,150))\n    print(img.shape)\n    ax[2].imshow(img)\n    ax[2].set_title('predicted image')\n    plt.tight_layout()\n    img=np.expand_dims(img,axis=0)\n    #making it model ready\n    \n    print(img.shape)\n    return img\n\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(url):\n    img=process_image(url)\n    label=model.predict(img)\n    final_1=np.argmax(label,axis=1)[0]\n    plt.xlabel(categories[final_1])\n    return categories[final_1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"https://media4.picsearch.com/is?LwsQDsAhRnF2IV-PP61f1fCUcQWD2jYoz6X55V_6-dg&height=266\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(\"https://media5.picsearch.com/is?8agnR1fAz2qzGkGmQsnFEb0nXkmuh-7hb-Il2rLLd7U&height=341\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}